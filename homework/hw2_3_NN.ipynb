{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics # Metrics for classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from statsmodels.stats import contingency_tables # Contringency tables\n",
    "import torch # PyTorch\n",
    "from torch import nn # Modules and layers\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset # PyTorch dataset\n",
    "from torch.utils.data import DataLoader # PyTorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import  MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv dataset\n",
    "df = pd.read_csv('data_assignment_1.csv')\n",
    "df.describe()\n",
    "\n",
    "# input\n",
    "feature_cols = ['Frontal_Sup', 'Frontal_Inf', 'Cingulum_Ant', 'Cingulum_Post', 'Parietal_Sup', 'Parietal_Inf', 'Occipital_Sup', 'Occipital_Inf', 'Temporal_Sup', 'Temporal_Inf']\n",
    "# feature_cols = ['Cingulum_Ant', 'Cingulum_Post']\n",
    "# output (label)\n",
    "target_col = ['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col):\n",
    "        # data loading\n",
    "        self.X = torch.tensor(df[feature_cols].values, dtype=torch.float32) # inputs: brain regional biomarkers\n",
    "        self.y = torch.tensor(df[target_col].values, dtype=torch.float32) # output: diagnosis (0 or 1)\n",
    "        # self.n_samples = len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # dataset[0]\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # len(data)\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "brain_data = BrainDataset(df, feature_cols, target_col)\n",
    "# print(brain_data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "batch_size = 25\n",
    "dataloaders = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(df):\n",
    "    train_df_kfold = df.iloc[train_idx]\n",
    "    test_df_kfold = df.iloc[test_idx]\n",
    "\n",
    "    train_ds_kfold = BrainDataset(train_df_kfold, feature_cols=feature_cols, target_col=target_col)\n",
    "    test_ds_kfold = BrainDataset(test_df_kfold, feature_cols=feature_cols, target_col=target_col)\n",
    "\n",
    "    train_dl_kfold = DataLoader(train_ds_kfold, batch_size=batch_size, shuffle=True)\n",
    "    test_dl_kfold = DataLoader(test_ds_kfold, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    dataloaders.append((train_dl_kfold, test_dl_kfold))\n",
    "\n",
    "# # --------------------------- TESTPRINT\n",
    "# # Retrieve the dataloader for one of the folds (e.g., the first fold)\n",
    "# train_dataloader, _ = dataloaders[3]  # getting the train dataloader for the first fold\n",
    "\n",
    "# # Retrieve one batch of data\n",
    "# batch = next(iter(train_dataloader))\n",
    "# inputs, outputs = batch\n",
    "\n",
    "# # Print or inspect the inputs and outputs\n",
    "# print(\"Input features for one batch:\", inputs)\n",
    "# print(\"Outputs for one batch:\", outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # divide the 'df' into training and testing\n",
    "# train_df = df.sample(frac = 0.75)\n",
    "# test_df = df.drop(train_df.index)\n",
    "# # prepare the datasets in the appropiate format 'ds'\n",
    "# train_ds = BrainDataset(train_df, feature_cols=feature_cols, target_col=target_col)\n",
    "# test_ds = BrainDataset(test_df, feature_cols=feature_cols, target_col=target_col)\n",
    "# # Create dataloaders\n",
    "# train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True) # Shuffle the train dataset!\n",
    "# test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_size = len(feature_cols) # Brain regions biomarkers value\n",
    "hidden_size = 70 # any number\n",
    "output_size = 1 # diagnosis (yer or no)\n",
    "\n",
    "class AdClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AdClassifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)  # First (input) layer\n",
    "        self.relu = nn.LeakyReLU()                         # nonlinear activation function\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size) # Second (hidden) layer\n",
    "        self.sigmoid = nn.Sigmoid()                        # Sigmoid for binary classification\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.flatten(x)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        output = self.sigmoid(out)\n",
    "        return output\n",
    "model = AdClassifier(input_size,hidden_size,output_size)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "learning_rate = 0.0001\n",
    "loss_func = nn.BCELoss()\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop(Other Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, optimizer, loss_func, num_epochs, print_loss_every=100, normalize=False):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_dl)\n",
    "        optimizer.zero_grad()\n",
    "        for X, y in train_dl:\n",
    "            \n",
    "            # Normalize data if flag is set\n",
    "            if normalize:\n",
    "                X = normalize_data(X)\n",
    "            # forward and loss\n",
    "            y_predicted = model(X)\n",
    "            \n",
    "            # print(y_predicted.dtype)\n",
    "\n",
    "            loss = loss_func(y_predicted, y)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            #update\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # accumulate loss for entire dataloader\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        if (epoch+1) % print_loss_every == 0:\n",
    "            print(f'epoch: {epoch+1}, loss: {avg_loss:.4f}')\n",
    "\n",
    "            # Print the weights and biases\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    print(f'{name} weights: {param.data}')\n",
    "                if 'bias' in name:\n",
    "                    print(f'{name} biases: {param.data}')\n",
    "\n",
    "        # for param in model.parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         if param.grad is not None:\n",
    "        #             print(param.grad.data.norm())\n",
    "        #         else:\n",
    "        #             print(\"Gradient is None for this parameter.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yuzho\\Documents\\psyc221_ML_neuroimaging\\homework\\hw2_3_NN.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yuzho/Documents/psyc221_ML_neuroimaging/homework/hw2_3_NN.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yuzho/Documents/psyc221_ML_neuroimaging/homework/hw2_3_NN.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m eval_every \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yuzho/Documents/psyc221_ML_neuroimaging/homework/hw2_3_NN.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_model(model, train_dl, optimizer, loss_func, num_epochs, eval_every)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "def eval_model(model, test_dl):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for X, y in test_dl:\n",
    "        pred = model(X)\n",
    "        y_true += list(y.detach().cpu().numpy())\n",
    "        y_pred += list(pred.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "    # accuracy = metrics.accuracy_score(targets, predictions)\n",
    "    # balanced_accuracy = metrics.balanced_accuracy_score(targets, predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"y_pred:, {y_pred},y_true:, {y_true} Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUC: {auc}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have already defined your model, optimizer, loss function etc.\n",
    "# # Store metrics over the 5 folds\n",
    "# num_epochs = 2000\n",
    "# accuracies = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# aurocs = []\n",
    "\n",
    "# # Train model on each k-fold\n",
    "# for train_dl_kfold, test_dl_kfold in dataloaders:\n",
    "#     # Assuming you have a function 'train_model' for training your model\n",
    "#     train_model(model, train_dl_kfold, optimizer, loss_func, num_epochs)\n",
    "    \n",
    "#     # Evaluate model\n",
    "#     print(\"Performance on the train set\")\n",
    "#     eval_model(model, train_dl_kfold)\n",
    "\n",
    "#     print(\"Performance on the test set\")\n",
    "#     eval_model(model, test_dl_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuzho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\yuzho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\yuzho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\yuzho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\yuzho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for MLP\n",
      "Average Accuracy over 5-folds: 0.64 ± 0.05\n",
      "Average Precision over 5-folds: 0.67 ± 0.12\n",
      "Average Recall over 5-folds: 0.61 ± 0.15\n",
      "Average AUROC over 5-folds: 0.66 ± 0.05\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuzho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# define classifier type\n",
    "classifier_type ='mlp'\n",
    "# classifier_type ='svm'\n",
    "# classifier_type ='logistic'\n",
    "\n",
    "# KFold splitter\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "data_splits = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(df):\n",
    "    train_df_kfold = df.iloc[train_idx]\n",
    "    test_df_kfold = df.iloc[test_idx]\n",
    "    data_splits.append((train_df_kfold, test_df_kfold))\n",
    "\n",
    "# Metrics containers\n",
    "accuracies_train = []\n",
    "precisions_train = []\n",
    "recalls_train = []\n",
    "aurocs_train = []\n",
    "accuracies_test = []\n",
    "precisions_test = []\n",
    "recalls_test = []\n",
    "aurocs_test = []\n",
    "\n",
    "\n",
    "for train_df, test_df in data_splits:\n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df[target_col].values\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df[target_col].values\n",
    "    \n",
    "    # Choose classifier based on switch\n",
    "    if classifier_type == 'mlp':\n",
    "        clf = MLPClassifier(hidden_layer_sizes = 70, max_iter=2000)\n",
    "    elif classifier_type == 'svm':\n",
    "        clf = SVC(kernel='linear',decision_function_shape='ovr',probability=True) # Set probability to True to use roc_auc_score\n",
    "    elif classifier_type == 'logistic':\n",
    "        clf = LogisticRegression(max_iter=2000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # # print mlp specs\n",
    "    # print(\"Optimizer:\", clf.solver)\n",
    "    # print(\"Loss Function:\", clf.loss)\n",
    "    # print(\"Learning Rate:\", clf.learning_rate_init)\n",
    "    # print(\"Layer Nodes:\", clf.hidden_layer_sizes)\n",
    "\n",
    "    # # print svm specs\n",
    "    # print(\"Kernel:\", clf.kernel)\n",
    "    # print(\"Decision Function Shape:\", clf.decision_function_shape)\n",
    "\n",
    "\n",
    "    # Predict\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate on train set\n",
    "    accuracies_train.append(accuracy_score(y_train, y_train_pred))\n",
    "    precisions_train.append(precision_score(y_train, y_train_pred))\n",
    "    recalls_train.append(recall_score(y_train, y_train_pred))\n",
    "    aurocs_train.append(roc_auc_score(y_train, y_train_pred))\n",
    "    # Evaluate on test set\n",
    "    accuracies_test.append(accuracy_score(y_test, y_test_pred))\n",
    "    precisions_test.append(precision_score(y_test, y_test_pred))\n",
    "    recalls_test.append(recall_score(y_test, y_test_pred))\n",
    "    aurocs_test.append(roc_auc_score(y_test, y_test_pred))\n",
    "\n",
    " # Calculate and report results for each classifier\n",
    "print(f\"Results for {classifier_type.upper()}\")\n",
    "print(\"Average Accuracy over 5-folds: {:.2f} ± {:.2f}\".format(np.mean(accuracies_test), np.std(accuracies_test)))\n",
    "print(\"Average Precision over 5-folds: {:.2f} ± {:.2f}\".format(np.mean(precisions_test), np.std(precisions_test)))\n",
    "print(\"Average Recall over 5-folds: {:.2f} ± {:.2f}\".format(np.mean(recalls_test), np.std(recalls_test)))\n",
    "print(\"Average AUROC over 5-folds: {:.2f} ± {:.2f}\".format(np.mean(aurocs_test), np.std(aurocs_test)))\n",
    "print(\"-----------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
