{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103201b0",
   "metadata": {},
   "source": [
    "# Machine Learning for Neuroimaging \n",
    "\n",
    "## Hands on Statistics & ML (Python) - 10/19/2023\n",
    "### Part 2: Classifier Training and Evaluation with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20154f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics # Metrics for classification\n",
    "from statsmodels.stats import contingency_tables # Contringency tables\n",
    "import torch # PyTorch\n",
    "from torch import nn # Modules and layers\n",
    "from torch.utils.data import Dataset # PyTorch dataset\n",
    "from torch.utils.data import DataLoader # PyTorch Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51ac88",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea75942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>A1_Score_prime</th>\n",
       "      <th>A2_Score_prime</th>\n",
       "      <th>A3_Score_prime</th>\n",
       "      <th>A4_Score_prime</th>\n",
       "      <th>A5_Score_prime</th>\n",
       "      <th>A6_Score_prime</th>\n",
       "      <th>A7_Score_prime</th>\n",
       "      <th>A8_Score_prime</th>\n",
       "      <th>A9_Score_prime</th>\n",
       "      <th>A10_Score_prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>703.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>351.926031</td>\n",
       "      <td>0.721195</td>\n",
       "      <td>0.453770</td>\n",
       "      <td>0.458037</td>\n",
       "      <td>0.496444</td>\n",
       "      <td>0.499289</td>\n",
       "      <td>0.284495</td>\n",
       "      <td>0.418208</td>\n",
       "      <td>0.650071</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755879</td>\n",
       "      <td>0.477765</td>\n",
       "      <td>0.456637</td>\n",
       "      <td>0.524960</td>\n",
       "      <td>0.520699</td>\n",
       "      <td>0.323009</td>\n",
       "      <td>0.386890</td>\n",
       "      <td>0.620102</td>\n",
       "      <td>0.319345</td>\n",
       "      <td>0.603399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>203.201765</td>\n",
       "      <td>0.448731</td>\n",
       "      <td>0.498213</td>\n",
       "      <td>0.498591</td>\n",
       "      <td>0.500343</td>\n",
       "      <td>0.500355</td>\n",
       "      <td>0.451495</td>\n",
       "      <td>0.493616</td>\n",
       "      <td>0.477287</td>\n",
       "      <td>0.468455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447490</td>\n",
       "      <td>0.495977</td>\n",
       "      <td>0.496188</td>\n",
       "      <td>0.496722</td>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.448073</td>\n",
       "      <td>0.492705</td>\n",
       "      <td>0.475392</td>\n",
       "      <td>0.464583</td>\n",
       "      <td>0.493442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004978</td>\n",
       "      <td>-0.132533</td>\n",
       "      <td>-0.137897</td>\n",
       "      <td>-0.140061</td>\n",
       "      <td>-0.012749</td>\n",
       "      <td>-0.131993</td>\n",
       "      <td>-0.061520</td>\n",
       "      <td>-0.095625</td>\n",
       "      <td>-0.197031</td>\n",
       "      <td>-0.072568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>176.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068227</td>\n",
       "      <td>0.029974</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.033673</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>-0.059586</td>\n",
       "      <td>-0.000606</td>\n",
       "      <td>-0.002144</td>\n",
       "      <td>0.050891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>352.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000915</td>\n",
       "      <td>0.084272</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>0.120803</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.082878</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.926315</td>\n",
       "      <td>0.041218</td>\n",
       "      <td>0.986752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>527.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066044</td>\n",
       "      <td>1.017473</td>\n",
       "      <td>1.000976</td>\n",
       "      <td>1.033614</td>\n",
       "      <td>1.016575</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.940326</td>\n",
       "      <td>0.982732</td>\n",
       "      <td>0.939252</td>\n",
       "      <td>1.025794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>703.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.069100</td>\n",
       "      <td>1.096773</td>\n",
       "      <td>1.061505</td>\n",
       "      <td>1.120803</td>\n",
       "      <td>1.045899</td>\n",
       "      <td>1.124376</td>\n",
       "      <td>1.000536</td>\n",
       "      <td>1.090287</td>\n",
       "      <td>1.075534</td>\n",
       "      <td>1.082833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    A1_Score    A2_Score    A3_Score    A4_Score    A5_Score  \\\n",
       "count  703.000000  703.000000  703.000000  703.000000  703.000000  703.000000   \n",
       "mean   351.926031    0.721195    0.453770    0.458037    0.496444    0.499289   \n",
       "std    203.201765    0.448731    0.498213    0.498591    0.500343    0.500355   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%    176.500000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%    352.000000    1.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%    527.500000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "max    703.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         A6_Score    A7_Score    A8_Score    A9_Score  ...  A1_Score_prime  \\\n",
       "count  703.000000  703.000000  703.000000  703.000000  ...      703.000000   \n",
       "mean     0.284495    0.418208    0.650071    0.324324  ...        0.755879   \n",
       "std      0.451495    0.493616    0.477287    0.468455  ...        0.447490   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...       -0.004978   \n",
       "25%      0.000000    0.000000    0.000000    0.000000  ...        0.068227   \n",
       "50%      0.000000    0.000000    1.000000    0.000000  ...        1.000915   \n",
       "75%      1.000000    1.000000    1.000000    1.000000  ...        1.066044   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...        1.069100   \n",
       "\n",
       "       A2_Score_prime  A3_Score_prime  A4_Score_prime  A5_Score_prime  \\\n",
       "count      703.000000      703.000000      703.000000      703.000000   \n",
       "mean         0.477765        0.456637        0.524960        0.520699   \n",
       "std          0.495977        0.496188        0.496722        0.499915   \n",
       "min         -0.132533       -0.137897       -0.140061       -0.012749   \n",
       "25%          0.029974        0.008382        0.044820        0.033673   \n",
       "50%          0.084272        0.051417        0.120803        0.045899   \n",
       "75%          1.017473        1.000976        1.033614        1.016575   \n",
       "max          1.096773        1.061505        1.120803        1.045899   \n",
       "\n",
       "       A6_Score_prime  A7_Score_prime  A8_Score_prime  A9_Score_prime  \\\n",
       "count      703.000000      703.000000      703.000000      703.000000   \n",
       "mean         0.323009        0.386890        0.620102        0.319345   \n",
       "std          0.448073        0.492705        0.475392        0.464583   \n",
       "min         -0.131993       -0.061520       -0.095625       -0.197031   \n",
       "25%          0.021552       -0.059586       -0.000606       -0.002144   \n",
       "50%          0.082878        0.000272        0.926315        0.041218   \n",
       "75%          0.963454        0.940326        0.982732        0.939252   \n",
       "max          1.124376        1.000536        1.090287        1.075534   \n",
       "\n",
       "       A10_Score_prime  \n",
       "count       703.000000  \n",
       "mean          0.603399  \n",
       "std           0.493442  \n",
       "min          -0.072568  \n",
       "25%           0.050891  \n",
       "50%           0.986752  \n",
       "75%           1.025794  \n",
       "max           1.082833  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv dataset\n",
    "dataframe = pd.read_csv('data/autism_screening_preprocessed.csv')\n",
    "dataframe.describe()\n",
    "\n",
    "# Notice that we have the residualized \"_prime\" features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968602c",
   "metadata": {},
   "source": [
    "## Autism Classification \n",
    "\n",
    "Train a model $f$ that given an input $x$ predicts a classification label $y$\n",
    "\n",
    "In this problem $x$ denotes the input features of the questionnaire and $y$ is the subject classification to control and ASD based on their responses\n",
    "\n",
    "First, prepare your data for PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3e65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for classification\n",
    "\n",
    "# The features we want to use to classify Control vs ASD are the responses to the autism questionnaire\n",
    "feature_cols = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', \n",
    "                               'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']\n",
    "# After the residualization!\n",
    "feature_cols = [col+'_prime' for col in feature_cols]\n",
    "\n",
    "target_col = 'Class/ASD'\n",
    "\n",
    "# Transform target column into a binary label\n",
    "labels = {\"NO\": 0, \"YES\": 1}\n",
    "dataframe = dataframe.replace({target_col: labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c98058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial dataframe had 703 individuals. Our train dataset contains 527 and our test dataset, 176.\n"
     ]
    }
   ],
   "source": [
    "# Divide the dataset into training and testing\n",
    "train_ds = dataframe.sample(frac = 0.75)\n",
    "test_ds = dataframe.drop(train_ds.index)\n",
    "\n",
    "# Sometimes, you need to pay attention to certain examples remaining together! \n",
    "# For instance, scans from the same subject at different timepoints, or siblings/members of the same family\n",
    "\n",
    "print(f\"The initial dataframe had {len(dataframe)} individuals. Our train dataset contains {len(train_ds)} and our test dataset, {len(test_ds)}.\")\n",
    "\n",
    "# If some classes are under-represented (often the case with multi-class problems), the subsets must \n",
    "# be stratified. I.e., the proportion of classes is maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a43bf",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db92d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a PandasDataset in the PyTorch format\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col, y_dtype=torch.int64):\n",
    "        self.X = torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(df[target_col].values, dtype=y_dtype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85181f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From that class, prepare the datasets in the appropiate format\n",
    "train_pt_ds = PandasDataset(train_ds, feature_cols=feature_cols, target_col=target_col)\n",
    "test_pt_ds = PandasDataset(test_ds, feature_cols=feature_cols, target_col=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ada6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example X: [[ 1.000697    0.01330662  1.0161082   1.017332    1.0012801   0.01325238\n",
      "   1.0001404   0.9899909   1.0197824   1.0072861 ]\n",
      " [ 0.9991692   0.9841387   0.98079944  0.9793406   0.99847424  0.9842034\n",
      "   0.99983263  1.0119307   0.97641975  0.9913152 ]\n",
      " [ 0.06058744 -0.06573388 -0.13789669 -0.09086361  0.03026628 -0.03746829\n",
      "  -0.06116859  1.0266112  -0.17559499 -0.00614747]\n",
      " [ 1.067572    0.06760496  1.0235145   1.0828112   1.0430927   1.0953271\n",
      "   0.9402383   0.92631495  1.0226341   0.06686208]\n",
      " [ 1.0013518   0.02580713  0.03124042  1.033614    0.00248251  0.02570195\n",
      "   1.0002723  -0.01941189  0.03836637  1.0141307 ]]\n",
      "Shape of X: torch.Size([5, 10])\n",
      "Example y: tensor([1, 1, 0, 1, 0])\n",
      "Shape of y: torch.Size([5]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Now build the dataloaders from those datasets\n",
    "batch_size = 5  # Number of samples on each iteration, select max. possible for the available memory\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = DataLoader(train_pt_ds, batch_size=batch_size, shuffle=True) # Shuffle the train dataset!\n",
    "test_dl = DataLoader(test_pt_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in train_dl:\n",
    "    print(f\"Example X: {X.numpy()}\")\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Example y: {y}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf88323",
   "metadata": {},
   "source": [
    "## PyTorch Models: building a linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07fc2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classifier in PyTorch\n",
    "class FullyConnectedClassifier(nn.Module):\n",
    "    def __init__(self, input_size=None, nr_classes=2, hidden_layer_sizes=[16,8], save_path=\"models\"):\n",
    "        super().__init__()\n",
    "        self.name = self.__class__.__name__ + '-'.join([str(n) for n in hidden_layer_sizes])\n",
    "        self.save_path = save_path\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        # Define the architecture of the model\n",
    "        layers = []\n",
    "        nr_neurons = [input_size] + hidden_layer_sizes\n",
    "        for ix in range(len(nr_neurons)-1):\n",
    "            # A linear, fully-connected layer\n",
    "            layers.append(nn.Linear(nr_neurons[ix], nr_neurons[ix+1]))\n",
    "            # A ReLU activation function\n",
    "            layers.append(nn.ReLU())\n",
    "        # Final layer, in this case for binary (nr_classes==2) classification\n",
    "        layers.append(nn.Linear(nr_neurons[-1], nr_classes))\n",
    "        # Finally, we place them one after the other\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # We'll sometimes need to flatten the inputs (not necessary in this case)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # The softmax function ensures we have one output per class, and these add up to 1\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Make a prediction based on a given input'''\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X)\n",
    "            return int(pred.argmax().detach())     \n",
    "        \n",
    "    def save(self, state_name='last', verbose=False):\n",
    "        '''Saves a model state in the defined path, with the model name'''\n",
    "        model_state_name = self.name+'_'+state_name+'.pth'\n",
    "        torch.save(self.state_dict(), os.path.join(self.save_path, model_state_name))\n",
    "        if verbose:\n",
    "            print(\"Saved PyTorch model state {} in {}\".format(model_state_name, self.save_path))\n",
    "            \n",
    "    def restore(self, state_name):\n",
    "        '''Restores a model state for the given state name'''\n",
    "        model_state_name = self.name+'_'+state_name+'.pth'\n",
    "        self.load_state_dict(torch.load(os.path.join(self.save_path, model_state_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb334da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedClassifier(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print out model summary\n",
    "model = FullyConnectedClassifier(input_size=len(feature_cols), hidden_layer_sizes=[2, 2])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13088a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to train and evaluate model\n",
    "def train_model(model, train_dl, optimizer, loss_f, nr_epochs, print_loss_every=10):\n",
    "    for t in range(nr_epochs):\n",
    "        model.train()\n",
    "        nr_batches = len(train_dl)\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for _, (X, y) in enumerate(train_dl):\n",
    "            # Backpropagation step\n",
    "            pred = model(X)\n",
    "            loss = loss_f(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        if t % print_loss_every == 0:\n",
    "            print(f\"Epoch {t} loss {total_loss / nr_batches}\")\n",
    "    model.save()\n",
    "    print(\"Finished training!\")\n",
    "            \n",
    "def eval_model(model, dl):\n",
    "    model.eval() # This is important for certain stochastic elements, such as MC Dropout\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    for X, y in dl:\n",
    "        pred = model(X)\n",
    "        targets += list(y.detach().cpu().numpy())\n",
    "        predictions += list(pred.argmax(1).detach().cpu().numpy())\n",
    "    accuracy = metrics.accuracy_score(targets, predictions)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(targets, predictions)\n",
    "    print(f\"Accuracy: {accuracy}, Balanced Accuracy: {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88e7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 0.6181832194890616\n",
      "Epoch 5 loss 0.522482619814153\n",
      "Epoch 10 loss 0.41012479786602957\n",
      "Epoch 15 loss 0.28879136163389907\n",
      "Epoch 20 loss 0.21716988797492856\n",
      "Epoch 25 loss 0.1701575539751365\n",
      "Epoch 30 loss 0.14011782219519242\n",
      "Epoch 35 loss 0.11939304139415535\n",
      "Epoch 40 loss 0.10901243030670196\n",
      "Epoch 45 loss 0.09452013022622988\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "learning_rate = 0.001\n",
    "nr_epochs = 50\n",
    "eval_every = 5\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_model(model, train_dl, optimizer, loss_f, nr_epochs, eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c02658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the train set\n",
      "Accuracy: 0.9810246679316889, Balanced Accuracy: 0.9804449521322889\n",
      "Performance on the test set\n",
      "Accuracy: 0.9261363636363636, Balanced Accuracy: 0.8774385072094996\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "print(\"Performance on the train set\")\n",
    "eval_model(model, train_dl)\n",
    "\n",
    "print(\"Performance on the test set\")\n",
    "eval_model(model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069806d",
   "metadata": {},
   "source": [
    "## Restoring and saving models\n",
    "\n",
    "### In order to make predictions in the future, we must save and restore model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b39b7dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the train set\n",
      "Accuracy: 0.3605313092979127, Balanced Accuracy: 0.2870430809399478\n",
      "Performance on the test set\n",
      "Accuracy: 0.375, Balanced Accuracy: 0.30296861747243425\n"
     ]
    }
   ],
   "source": [
    "# Initialize new model, and evaluate\n",
    "model = FullyConnectedClassifier(input_size=len(feature_cols), hidden_layer_sizes=[2, 2])\n",
    "print(\"Performance on the train set\")\n",
    "eval_model(model, train_dl)\n",
    "\n",
    "print(\"Performance on the test set\")\n",
    "eval_model(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a708e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the train set\n",
      "Accuracy: 0.9810246679316889, Balanced Accuracy: 0.9804449521322889\n",
      "Performance on the test set\n",
      "Accuracy: 0.9261363636363636, Balanced Accuracy: 0.8774385072094996\n"
     ]
    }
   ],
   "source": [
    "# Now, let's restore our previous model state and evaluate anew\n",
    "model.restore(state_name=\"last\")\n",
    "\n",
    "print(\"Performance on the train set\")\n",
    "eval_model(model, train_dl)\n",
    "\n",
    "print(\"Performance on the test set\")\n",
    "eval_model(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bc60491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedClassifier(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Now let's build a larger model with 2 hidden layers\n",
    "model_big = FullyConnectedClassifier(input_size=len(feature_cols), hidden_layer_sizes=[8, 16, 8])\n",
    "print(model_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "202278af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 0.6419557864373585\n",
      "Epoch 5 loss 0.20466581948501286\n",
      "Epoch 10 loss 0.12105143312000675\n",
      "Epoch 15 loss 0.08235919920038351\n",
      "Epoch 20 loss 0.05592728185962977\n",
      "Epoch 25 loss 0.04390127951876728\n",
      "Epoch 30 loss 0.028247982822702036\n",
      "Epoch 35 loss 0.020187934383667016\n",
      "Epoch 40 loss 0.015162830904238481\n",
      "Epoch 45 loss 0.012960436522536263\n",
      "Finished training!\n",
      "Performance on the train set\n",
      "Accuracy: 1.0, Balanced Accuracy: 1.0\n",
      "Performance on the test set\n",
      "Accuracy: 0.9829545454545454, Balanced Accuracy: 0.973960983884648\n"
     ]
    }
   ],
   "source": [
    "# We will train that model for the same number of epochs\n",
    "optimizer = torch.optim.Adam(model_big.parameters(), lr=learning_rate)\n",
    "train_model(model_big, train_dl, optimizer, loss_f, nr_epochs, eval_every)\n",
    "\n",
    "print(\"Performance on the train set\")\n",
    "eval_model(model_big, train_dl)\n",
    "\n",
    "print(\"Performance on the test set\")\n",
    "eval_model(model_big, test_dl)\n",
    "\n",
    "# The mdoels are different, but are they significantly different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3619d7c",
   "metadata": {},
   "source": [
    "## *Null hypothesis*: the two ML models are equally accurate on the same cohort\n",
    "\n",
    "\n",
    "## McNemar's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c52a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract predictions from the two models\n",
    "model.eval()\n",
    "model_big.eval()\n",
    "y_true = []\n",
    "y_1 = []\n",
    "y_2 = []\n",
    "for X, y in test_dl:\n",
    "    pred_1 = model(X)\n",
    "    y_1 += list(pred_1.argmax(1).detach().cpu().numpy())\n",
    "    pred_2 = model_big(X)\n",
    "    y_2 += list(pred_2.argmax(1).detach().cpu().numpy())\n",
    "    y_true += list(y.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7afc6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate contingency matrix\n",
    "def contingency_table_calc(y_true, y_1, y_2):\n",
    "    y_true, y_1, y_2 = np.array(y_true), np.array(y_1), np.array(y_2)\n",
    "    c1i_c2c = sum(np.logical_and((y_1 != y_true),(y_2 == y_true)))\n",
    "    c1c_c2i = sum(np.logical_and((y_1 == y_true),(y_2 != y_true)))\n",
    "    c1c_c2c = sum(np.logical_and((y_1 == y_true),(y_2 == y_true)))\n",
    "    c1i_c2i = sum(np.logical_and((y_1 != y_true),(y_2 != y_true)))\n",
    "\n",
    "    contingency_table = [[c1c_c2c, c1c_c2i],[c1i_c2c, c1i_c2i]]\n",
    "\n",
    "    return contingency_table\n",
    "\n",
    "contingency_table = contingency_table_calc(y_true, y_1, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b42ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Hypothesis: the two ML models are equally accurate on the same cohort.\n",
      "p-value with NcNemars exact test: 0.0020\n",
      "p<0.05 --> we reject the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# Compute McNemar statistic and p-value to compare the two models\n",
    "p = contingency_tables.mcnemar(contingency_table, exact=True).pvalue\n",
    "\n",
    "print(\"Null Hypothesis: the two ML models are equally accurate on the same cohort.\")\n",
    "print(f'p-value with NcNemars exact test: {p:.4f}')\n",
    "if p < 0.05:\n",
    "    print(\"p<0.05 --> we reject the null hypothesis.\")\n",
    "else:\n",
    "    print(\"p>0.05 --> we cannot reject the null hypothesis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
